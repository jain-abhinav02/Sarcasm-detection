{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chandler 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcuoEdBWKuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e92ad14-6f83-4652-b5b4-a4b85bccaae0"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,GRU,Dropout,BatchNormalization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Embedding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.initializers import Constant\n",
        "import pandas as pd\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Bidirectional\n",
        "from keras.optimizers import adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sklearn\n",
        "from keras.layers import Activation\n",
        "from sklearn import model_selection\n",
        "from keras import utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9YfXbpYWu7t",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPDpQWA1WxJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df0 = pd.read_csv('train.csv')\n",
        "df1= pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDUUYbgaW1GH",
        "colab_type": "text"
      },
      "source": [
        "## Output labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAqIF5fW3rM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df0['label'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoEd15TwW5iR",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_KGNtCbW8oI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0510d4a5-b9c5-4528-9834-ed497e116fa7"
      },
      "source": [
        "maxlen= 70\n",
        "\n",
        "ser0=df0.text\n",
        "ser1=df1.text\n",
        "ser3 = pd.concat([ser0, ser1], ignore_index=True)\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(ser3)\n",
        "x_seq0 = tk.texts_to_sequences(ser0)\n",
        "x_seq1 = tk.texts_to_sequences(ser1)\n",
        "x_pad0 = pad_sequences(x_seq0,maxlen=maxlen)\n",
        "x_pad1 = pad_sequences(x_seq1,maxlen=maxlen)\n",
        "\n",
        "# Find the maximum length of any review\n",
        "print('Maximum review length: {}'.format(len(max(x_seq0, key=len))))\n",
        "\n",
        "vocab_size = len(tk.word_counts.keys())+1\n",
        "\n",
        "# Counting the number of unique words in all reviews\n",
        "word_index = tk.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 44\n",
            "Found 37645 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w7nRqbnW-g6",
        "colab_type": "text"
      },
      "source": [
        "## GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cr4ravEXCda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "520f35c4-caec-4aca-cda2-c144f57800c9"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-23 06:54:57--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2019-10-23 06:55:03--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2019-10-23 06:55:03--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  2.14MB/s    in 14m 34s \n",
            "\n",
            "2019-10-23 07:09:38 (2.05 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n",
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: glove.42B.300d.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waBojBGHX2HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d85565a1-fb7d-4671-cd8c-e419de19a89e"
      },
      "source": [
        "# processing the glove embedding txt file\n",
        "GLOVE_DIR = ''\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1917494 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57kKcqy3X5SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NUM_WORDS = 25000\n",
        "embedding_size = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGyHzOF8X14j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preparing embedding matrix\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, embedding_size))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8mlzMYzXFmo",
        "colab_type": "text"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HynvB5XzXKVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting training and test data\n",
        "x_train,x_test,y_train,y_test = train_test_split(x_pad0 , y , test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCYgkDvQX8x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting training and validation data\n",
        "valid_size = 2048\n",
        "x_train1 = x_train[valid_size:]\n",
        "y_train1 = y_train[valid_size:]\n",
        "x_val = x_train[:valid_size]\n",
        "y_val = y_train[:valid_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soDe8PwxXNOJ",
        "colab_type": "text"
      },
      "source": [
        "## Beginning the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGE45tR3XRXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"weights.{epoch:02d}-{val_loss:.2f}.h5,\"\n",
        "checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False,save_weights_only=True)\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnBrlF1TXV77",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4TZDs-gXXJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a RNN model\n",
        "def build_model(units1, units2 , units3, drop1=0.4,  drop2=0.4):\n",
        "  model = Sequential()\n",
        "  \n",
        "  # Embedding Layer\n",
        "  model.add(Embedding(vocab_size,embedding_size,embeddings_initializer=Constant(embedding_matrix),input_length = maxlen,trainable=True))\n",
        "  \n",
        "  # Layer 1\n",
        "  model.add(Bidirectional(LSTM(units1,return_sequences=False)))\n",
        "  model.add(Dropout(drop1))\n",
        "\n",
        "  # Layer 2\n",
        "  model.add(Dense(units2));\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(drop2))\n",
        "\n",
        "  # Layer 3\n",
        "  model.add(Dense(units3))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU());\n",
        "  model.add(Dropout(drop2))\n",
        "\n",
        "  # Layer 4\n",
        "  model.add(Dense(1,activation = 'sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oJZJfhLXdTb",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geJr8JOpXgT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "98c983df-c0f7-4f66-816f-b235ab4610d6"
      },
      "source": [
        "model = build_model(200,70,20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uj2RBNsYSah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "fa8f9c21-8b1e-4aeb-815a-a04a84584c53"
      },
      "source": [
        "# begin training on dataset\n",
        "batch_size = 512\n",
        "num_epochs = 6\n",
        "model.fit(x_train1,y_train1,validation_data= (x_val,y_val),batch_size = batch_size, epochs= num_epochs,callbacks=callbacks_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 29776 samples, validate on 2048 samples\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "29776/29776 [==============================] - 34s 1ms/step - loss: 0.6880 - acc: 0.6104 - val_loss: 0.5601 - val_acc: 0.7144\n",
            "\n",
            "Epoch 00001: saving model to weights.01-0.56.h5,\n",
            "Epoch 2/6\n",
            "29776/29776 [==============================] - 26s 880us/step - loss: 0.5261 - acc: 0.7409 - val_loss: 0.5613 - val_acc: 0.7305\n",
            "\n",
            "Epoch 00002: saving model to weights.02-0.56.h5,\n",
            "Epoch 3/6\n",
            "29776/29776 [==============================] - 26s 880us/step - loss: 0.4060 - acc: 0.8232 - val_loss: 0.4965 - val_acc: 0.7671\n",
            "\n",
            "Epoch 00003: saving model to weights.03-0.50.h5,\n",
            "Epoch 4/6\n",
            "29776/29776 [==============================] - 26s 882us/step - loss: 0.3134 - acc: 0.8726 - val_loss: 0.4860 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00004: saving model to weights.04-0.49.h5,\n",
            "Epoch 5/6\n",
            "29776/29776 [==============================] - 26s 881us/step - loss: 0.2411 - acc: 0.9088 - val_loss: 0.5477 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00005: saving model to weights.05-0.55.h5,\n",
            "Epoch 6/6\n",
            "29776/29776 [==============================] - 27s 893us/step - loss: 0.1904 - acc: 0.9314 - val_loss: 0.6068 - val_acc: 0.7866\n",
            "\n",
            "Epoch 00006: saving model to weights.06-0.61.h5,\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd0748e518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGyk-1KUZm3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX8BnhQSYSNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8eeb685d-02b1-4e81-d5d2-7e2e7fada4e0"
      },
      "source": [
        "# check accuracy on test data\n",
        "scores = model.evaluate(x_test,y_test,verbose=1)\n",
        "print(scores)\n",
        "print(\"accuracy:\",scores[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7956/7956 [==============================] - 29s 4ms/step\n",
            "[0.5703581128065283, 0.7926093514628479]\n",
            "accuracy: 0.7926093514628479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qte221bfXjef",
        "colab_type": "text"
      },
      "source": [
        "## Submission prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo3BUwpvXn0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 1975\n",
        "l=[i for i in range(length)]\n",
        "\n",
        "y_sub = model.predict(x_pad1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amOWUtK_XvRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_sub1 = y_sub >= 0.575\n",
        "y_sub2 = y_sub1.reshape((1975,))\n",
        "y_sub3=y_sub2.astype('int64')\n",
        "\n",
        "submission=pd.DataFrame({'Id':l,'label':y_sub3})\n",
        "submission.to_csv(\"submission.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f-AO_GHYgqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare with prev best submission\n",
        "sub0 = pd.read_csv('submission(23).csv')\n",
        "sub3 = pd.read_csv('submission.csv')\n",
        "cmp = sub0['label'] != sub3['label']\n",
        "cmp = cmp.to_numpy()\n",
        "print(np.sum(cmp,axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}